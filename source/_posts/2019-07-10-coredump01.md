---
layout: post
title:  "一次生产故障引发的思考"
date:	2019-07-10 21:07:11 +0800
categories: java
---

> 一个月以前我负责的系统突然发生了两次严重的宕机事故，这里总结一下事情的来龙去脉，梳理一下事故的原因和因此而获得的宝贵经验

### 事故经过

> 6月14号，星期五下班前夕，大家都沉浸在马上要过周末的兴奋之中。
>
> 17：30
>
> 突然收到运维工程师的一条消息：mmkt系统宕机了。mmkt由3个节点组成：A,B,C
>
> 17：31 
>
> 收到这个信息，直觉就是是不是有异常的流量，很快自己就否定了这个假设：因为mmkt这个系统是一个消息系统，就算有异常流量，消息队列也会起到消峰的作用。然后我尝试连接C服务器看看服务器的各项性能指标和系统运行日志奇怪的一幕发生了：SSH竟然连不了，提示：**-bash: fock : Cannot allocate memory**. 三台服务器中有一台出现这种情况。但是另外两台也不能正常提供服务了。
>
> 17：35 
> 尝试连接A机器，这次连上了，操作系统的内存，CPU，网络，IO都很正常，没有什么异常情况，查看mmkt的运行日志，发现大量的异常： **java.lang.OutOfMemoryError:Unable to create new native thread**
>
> 难道程序中有大量创建线程吗，脑海中马上回忆了一下mmkt系统中用到的线程池和对外的API，前者有最大线程数的限制，后者有熔断机制，都不会引发这种异常，那到底是什么原因呢？
>
> 17：40
>
> 运维工程师再次通知，A,B,C三台机器全部宕机了，而且SSH都已经不能连接，操作系统发生了coredump.也就是说现在这三台机器上具体到底发生了什么，大家一点头绪都没有。
>
> 17：45
>
> 经过大家的讨论，决定先联系数据中心对A,B,C三台机器进行物理重启，因为这是唯一的方法。
>
> 17：47
>
> 数据中心对A,B,C三台机器重启之后，A,B,C三台机器上的服务逐步恢复，本来以为系统已经正常了，但是很快相同的问题又出现了。这时候大家额头都开始冒汗。眼睁睁的看着系统再次宕机，紧接着大家进行了简单的讨论：结论是先对三台机器进行物理重启，然后暂停mmkt，这次三台机器没有再发生coredump
>
> 18：00
>
> 机器是暂时稳定了，但是mmkt系统依然处于停止状态，现在的当务之急是恢复mmkt。关键是到底是什么问题引发了**java.lang.OutOfMemoryError:Unable to create new native thread** ，人们总是对自己不太了解的技术缺乏信息，mmkt系统使用了一项新的技术：ignite，对于ignite的很多底层的细节知之甚少，ignite的数据持久化也是2个月以来系统的唯一变化点，所以，我们推测是ignite的数据持久化导致了这次生产故障。
>
> 18：05
>
> 向架构师汇报了我们的判断之后，架构师发话：尽快解决问题，一定要在21：00之前解决。接下来我们就进入了紧张的编码，调试，测试阶段。
>
> 19：00
>
> 代码终于改好了，上线了。这次mmkt启动正常了。大伙终于放心了，接下来的事情就是彻底搞清楚问题的原因，在测试环境重新的任务了。
>
> **正在大家以为问题已经解决了......**
>
> 一周之后
>
> 6月22号
>
> 正是周末，突然微信群炸锅了，mmkt再次发生宕机故障，接着就是coredump。相同的现象，相同的过程再次重新，这次大家都傻眼了，这是怎么回事？ignite的问题不是已经解决了吗？
>
> 
