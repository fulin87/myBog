---
layout: post
title:  "python爬虫框架crapy"
date:	2018-10-04 13:07:11 +0800
categories: python
---

> scrapy是python中最著名的爬虫软件，最近需要写爬虫，所以使用了这个框架，这里记录一下scrapy的知识点

### scrapy的命令

	scrapy startproject demo 	//生成一个scrapy项目
	scrapy genspider -l  		//列出scrapy的默认爬虫模板
	scrapy genspider -t basic baidu baidu.com //使用basic模板生成一个爬虫
	scrapy check 				//对代码进行检查
	scrapy list					//列出所有的爬虫
	scrapy fetch				//抓取一个网页
	scrapy fetch --nolog --headers http://www.baidu.com //抓取一个网页的信息
	scrapy view http://taobao.com //下载一个网页，用于调试，可见哪些是动态加载的
	scrapy shell http://taobao.com //用于调试的命令行，非常方便
	scrapy version -v 			//输出一些scrapy依赖库的版本
	scrapy bench http://taobao.com //测试当前硬件的爬行速度

用scrapy爬取天猫数据的时候，发现天猫的反爬机制非常强大，接口隐藏的很深，爬取一段时间后竟然要求登录，为了绕开他的反爬机制，使用了selenium来模拟浏览器的行为

## 用selenium爬取天猫商品详情页的价格

需求：给定一个天猫详情页的url，返回详情页的价格

实现：flask实现web服务，提供rest接口，接收请求后使用selenium模拟浏览器，然后抓取页面数据

	#! python2
	# coding: UTF-8
	'''
	Created on 2018年10月4日
	
	@author: fulin16
	'''
	
	from flask import Flask,request
	import json
	import requests
	
	from selenium import webdriver
	from selenium.webdriver.support.wait import WebDriverWait
	from selenium.webdriver.common.by import By
	from selenium.webdriver.support import expected_conditions as EC
	import time
	
	app = Flask(__name__)
	# browser = webdriver.Chrome()
	
	'''
	    code : 0,正常;1,请求方式不正确;2,获取请求参数异常
	    data : 根据url爬取到的数据
	    msg  : 响应描述信息
	'''
	@app.route("/getPrice",methods=['GET','POST','PUT'])
	def resaveUrl():
	    url = ''
	    result = {
	        'code':0,
	        'data':'',
	        'msg':''
	    }
	    try:
	        if request.method == 'GET':
	            #使用的是get请求
	            url = request.args.get('url', '')
	        elif request.method == 'POST':
	            #使用的是post请求
	            if not request.json:
	                #post过来的数据不是json
	                url =  "不是json"
	            else:
	                print "是json"
	                #post过来的数据是json
	                data = request.get_data()
	                url = json.loads(data)['url']
	        else:
	            url = ''
	            result['code'] = 1
	            result['msg'] = '只支持GET或者POST请求方式'
	            return json.dumps(result,separators=(',',':'),ensure_ascii=False)
	    except Exception :
	        result['code'] = 2
	        result['msg'] = '爬虫系统获取请求参数出现异常'
	        return json.dumps(result,separators=(',',':'),ensure_ascii=False)
	    
	    #根据URL进行数据的爬取
	    if(url.startswith("https://detail.tmall.com")):
	        #天猫
	        result['data'] = spiderTianMaoWithSelenuim(url)
	        #result['data'] = spiderTianMao(url)
	    return json.dumps(result,separators=(',',':'),ensure_ascii=False)
	
	#通过接口来爬取天猫
	def spiderTianMao(url):
	    splitList = url.split('&')
	    pId = splitList[0].split('id=')[1]
	    detailUrl = "https://mdskip.taobao.com/core/initItemDetail.htm?isUseInventoryCenter=false&cartEnable=true&service3C=false&isApparel=true&isSecKill=false&tmallBuySupport=true&isAreaSell=false&tryBeforeBuy=false&offlineShop=false&itemId="+ pId +"&showShopProm=false&isPurchaseMallPage=false&isRegionLevel=false&household=false&sellerPreview=false&queryMemberRight=true&addressLevel=2&isForbidBuyItem=false"
	#     detailUrl = detailUrl.format({'id':pId})
	    print detailUrl
	    headers={
	        'Referer':url,
	        'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',
	        'Accept-Language':'zh-CN,zh;q=0.9',
	        'Accept-Encoding':'gzip,deflate,br'
	    }
	    response = requests.get(url=detailUrl,headers=headers,verify=False)
	    print response
	    print response.content
	    return response.content
	    
	
	#通过selenuim爬取天猫数据
	def spiderTianMaoWithSelenuim(url):
	    browser = webdriver.PhantomJS(service_args=['--load-images=false','--disk-cache=true','--ignore-ssl-errors=true', '--ssl-protocol=TLSv1'])
	    browser.set_window_size(1400,900)
	    browser.get(url)
	    #time.sleep(2)
	    browser.find_element_by_xpath('//*[@id="sufei-dialog-close"]').click()
	    browser.execute_script("window.scrollBy(0,3000)")
	    browser.execute_script("window.scrollBy(0,-3000)")
	    price = WebDriverWait(browser,20).until(
	        EC.presence_of_element_located((By.XPATH,'//*[@id="J_StrPriceModBox"]/dd/span'))
	    )
	    priceValue = price.text
	    print priceValue
	    browser.close()
	    return priceValue
	
	if __name__ == '__main__':
	    app.run()
